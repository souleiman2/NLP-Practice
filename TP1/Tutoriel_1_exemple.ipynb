{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWpHzhobWbDG"
   },
   "source": [
    "# Tutoriel 1 - Modèle Bag of Words (BOW) à l'aide d'NLTK et pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EK8jLNxdd6-M"
   },
   "source": [
    "### 0. Téléchargement d'un livre éléctronique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-yhXA_2WWf4",
    "outputId": "068f3d75-838d-4fab-e7a4-61e197bb47ec"
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "print(raw[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqZCGqk7eAds"
   },
   "source": [
    "### 1. Tokenization de phrases et création d'un DataFrame Pandas\n",
    "\n",
    "On utilise le Sentence Tokenizer d'NLTK pour séparer le text brut en phrases. Chaque phrase ainsi obtenue correspond à une rangée du DataFrame.\n",
    "\n",
    "df.head() permet d'afficher les premières rangées (5 par défaut) d'un DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "Yt_i2so_ZSfo",
    "outputId": "8665f7dd-90a9-48f6-8c74-679c74bdf46c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "df = pd.DataFrame({\"documents\": sent_tokenize(raw)})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gk0oGnMbeWfp"
   },
   "source": [
    "### 2. Tokenization de mots et fonction apply\n",
    "\n",
    "Les documents (phrases) sont séparés en token à l'aide de l'outil word_tokenize. Cette opération est vectorisée et appliquée à l'ensemble des lignes du DataFrame avec l'opération \"apply\".\n",
    "\n",
    "À noter que dans ce contexte, la valeur assignée à la colonne \"words\" d'une rangée correspond à celle retournée par la fonction word_tokenize si on lui passait la valeur \"documents\" de la même rangée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "JTVNXW0_aRR1",
    "outputId": "d751ff22-0c9c-4e33-cd92-51213fa55d8f"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df[\"words\"] = df[\"documents\"].apply(word_tokenize)\n",
    "print(df[\"words\"][0])\n",
    "print(type(df[\"words\"][0]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9Sz6rqHe5z3"
   },
   "source": [
    "### 3. Retrait des stopwords\n",
    "\n",
    "Une fonction lambda est appliquée sur la colonne \"words\" de chaque rangée avec apply. La fonction lambda permet de manipuler explicitement la valeur existante d'une rangée.\n",
    "\n",
    "La manipulation effectuée dans cet exemple est la création d'une liste de mots sans stopwords, en reprenant chaque élément de la colonne \"words\" et ne gardant ceux ne figurant pas dans la liste des stopwords d'NLTK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "IVrRmjamaXcf",
    "outputId": "e1b3fc04-9026-4279-8dc7-0262732e1d4e"
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "df[\"stopwords_removed\"] = df[\"words\"].apply(lambda words: [word for word in words if word not in stop_words])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWTb0tGdf593"
   },
   "source": [
    "### 4. Stemming et Lemmatization\n",
    "\n",
    "Utilisation des librairies de NLTK pour stemmer ou lemmatizer les listes de mots de chaque phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyhQrUDMcV_I"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "df[\"porter_stemmed\"] = df[\"stopwords_removed\"].apply(lambda words: [ps.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "melLmO4sc7HA",
    "outputId": "3b30ed49-c4a8-4844-ab41-d9c5eed7efac"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "df[\"word_net_lemm\"] = df[\"stopwords_removed\"].apply(lambda words: [lem.lemmatize(word) for word in words])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QF5bnC-kU7K"
   },
   "source": [
    "### 5. Modèle Bag of Words\n",
    "\n",
    "Calcul de la fréquence des tokens dans le corpus, dont on garde les N (200 dans l'exemple) token les plus fréquents.\n",
    "\n",
    "Les phrases sont représentées par un vecteur de taille N: à chaque indice, 1 indique que ce Nième mot le plus fréquent est dans ladite phrase, 0 indique qu'il est absent de la phrase.\n",
    "\n",
    "Notez l'utilisation de la fonction \"value_counts\" sur une série Pandas; celle-ci retourne un décompte des valeurs les plus fréquentes d'une colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eP5hHCxllayn"
   },
   "outputs": [],
   "source": [
    "# décompte de tous les tokens dans le corpus entier\n",
    "vocab_words = pd.Series(word_tokenize(raw))\n",
    "vocab_words = [word for word in vocab_words if word not in stop_words]\n",
    "vocab_words = [lem.lemmatize(word) for word in vocab_words]\n",
    "\n",
    "# 200 mots les plus fréquents\n",
    "token_frequencies = pd.Series(vocab_words).value_counts().head(200).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyGrdWiIl2rV"
   },
   "outputs": [],
   "source": [
    "# 1 si le token parmi les plus fréquents est dans la phrase, 0 sinon\n",
    "def sentence_vector(sentence, token_frequencies):\n",
    "  return [1 if token in sentence else 0 for token in token_frequencies.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "_T2AO70vj1v3",
    "outputId": "4195afa9-4e7c-4a0b-f9b4-271e7e0e0194"
   },
   "outputs": [],
   "source": [
    "df[\"BOWrepresentation\"] = df.word_net_lemm.apply(lambda x: sentence_vector(x, token_frequencies))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN35-xNPoIv6"
   },
   "source": [
    "#### Représentation des BOW en matrice numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8Hf4HFRdvrY",
    "outputId": "59cb0181-94fd-4959-be69-714904c2d155"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.asarray(list(df.BOWrepresentation.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-zqDC891_bF"
   },
   "source": [
    "#### 6. Modèle TF-IDF avec sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBsfkuNe2Ee7",
    "outputId": "f0718c1a-626c-4c0b-81e7-b3f0e9237878"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "df[\"joined_lemmas\"] = df.word_net_lemm.apply(lambda x: \" \".join(x))\n",
    "\n",
    "tfidf = vectorizer.fit_transform(df[\"joined_lemmas\"].values)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "tfidf = tfidf.toarray()\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT-JR8f4fHsb"
   },
   "source": [
    "### 7. Distance entre deux représentations de documents\n",
    "\n",
    "La librairie scipy offre des outils pour les calculs de distance entre vecteurs unidimensionnels. Par exemple, voici comment calculer la distance cosinus entre les représentations TF-IDF des deux premières phrases du livre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQTQp8I3fkWg",
    "outputId": "4dc0f664-bf1d-4be7-8873-d7715e188f2c"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "cosine(tfidf[0], tfidf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfcyvqbVfFYz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exemples_tutoriel_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
