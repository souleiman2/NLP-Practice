{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## <center> École Polytechnique de Montréal <br> Département Génie Informatique et Génie Logiciel <br>  INF8460 – Traitement automatique de la langue naturelle <br> </center>\n",
    "## <center> TP1 INF8460 <br>  Automne 2021 </center>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. DESCRIPTION\n",
    "Dans ce TP, l’idée est d’effectuer de la recherche de passages de texte dans un corpus à partir d’une question en langue naturelle. Les questions et passages sont en anglais.\n",
    "\n",
    "Voici un exemple : <br>\n",
    "__Entrée : Question :__ What causes precipitation to fall?  \n",
    "\n",
    "__Solution - Trouver un passage qui contient la réponse à la question :__ In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under <mark> __gravity__ </mark>. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called “showers”. \n",
    "\n",
    "Ici la réponse est en gras dans le texte."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. LIBRARIES PERMISES\n",
    "- Jupyter notebook\n",
    "- NLTK\n",
    "- Numpy \n",
    "- Pandas\n",
    "- Sklearn\n",
    "- Pour toute autre librairie, demandez à votre chargé de laboratoire\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. INFRASTRUCTURE\n",
    "\n",
    "- Vous avez accès aux GPU du local L-4818. Dans ce cas, vous devez utiliser le dossier temp (voir le tutoriel VirtualEnv.pdf)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. DESCRIPTION DES DONNEES\n",
    "\n",
    "Dans ce projet, vous utiliserez le jeu de données dans le répertoire _data_. Il est décomposé en données d’entrainement (train), de validation (dev) et de test (test). <br>\n",
    "\n",
    "Nous ne mettrons à votre disposition que les données d’entrainement et de validation. Les données de test ne contiennent pas le paragraphe de réponse et doivent être complétées avec les résultats de votre système.\n",
    "Nous vous fournissons un ensemble de données qui comprend un corpus (_corpus.csv_) qui contient tous les passages et leurs identificateurs (ID) et un jeu de données qui associe une question, un passage, et une réponse qui est directement extraite du passage. Notez que certains passages contiennent des balises HTML et qu’il vous faudra procéder à un prétraitement de ces passages pour les enlever. <br>\n",
    "Ce jeu de données est composé de trois sous-ensembles : \n",
    "- _Train_ : ensemble d’entraînement de la forme <QuestionID, QuestionText, PassageID, Réponse>. Le but est donc d’entrainer votre modèle à retrouver le passage qui contient la réponse à la question.\n",
    "- _Validation_ : De la même forme que le Train, il vous permet de valider votre entraînement et de tester les performances de certains modules.  \n",
    "- _Test_ : Un ensemble secret qui est utilisé pour évaluer votre système complet. Il est de la forme <QuestionID, Question>. Votre système doit trouver dans le corpus __corpus.csv__ le ou les passages les plus pertinents.\n",
    "\n",
    "Notez qu’il est possible de répondre aux requis du TP sans utiliser la réponse à la question. C’est à vous de choisir si vous utilisez la réponse ou non. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. ETAPES DU TP \n",
    "A partir du notebook _inf8460_A21_TP1_ qui est distribué, vous devez réaliser les étapes suivantes. (Noter que les cellules dans le squelette sont là à titre informatif - il est fort probable que vous rajoutiez des sections au fur et à mesure de votre TP)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ci-dessous définir la constante _PATH_ qui doit être utilisée par votre code pour accéder aux fichiers. Il est attendu que pour la correction, le chargé de lab n'ait qu'à changer la valeur de _PATH_ pour le répertoire où se trouver les fichiers de datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "source": [
    "\"\"\" # For ubuntu\r\n",
    "import sys\r\n",
    "!{sys.executable} -m pip install pandas\r\n",
    "!{sys.executable} -m pip install nltk\r\n",
    "!{sys.executable} -m pip install sklearn\r\n",
    "!{sys.executable} -m pip install numpy\r\n",
    "import ssl\r\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\r\n",
    "\"\"\"\r\n",
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download(\"stopwords\")\r\n",
    "nltk.download(\"wordnet\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FabriceNdui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FabriceNdui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\FabriceNdui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 455
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "source": [
    "PATH = \"data/\""
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "import pandas as pd\r\n",
    "from nltk.tokenize import sent_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import numpy as np\r\n",
    "import nltk\r\n",
    "import re\r\n",
    "import string\r\n",
    "\r\n",
    "nltk.download(\"stopwords\")\r\n",
    "nltk.download(\"wordnet\")\r\n",
    "lem = WordNetLemmatizer()\r\n",
    "ps = PorterStemmer()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FabriceNdui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\FabriceNdui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1. Pré-traitement (12 points)\n",
    "Les passages et questions de votre ensemble de données doivent d’abord être représentés et indexés pour ensuite pouvoir effectuer une recherche de passage pour répondre à une question. On vous demande donc d’implémenter une étape de pré-traitement des données.\n",
    "1) (_6 points_) Complétez les fonctions retournant les informations suivantes (une fonction par information, chaque fonction prenant en argument le corpus (passages, questions) composé d'une liste de phrases segmentées en jetons/tokens) :\n",
    "    1. Le nombre total de jetons (mots non distincts)\n",
    "    2. Le nombre total de mots distincts (les types qui constituent le vocabulaire)\n",
    "    3. Les N mots les plus fréquents du vocabulaire (N est un paramètre avec une valeur par défaut de 10) ainsi que leur fréquence\n",
    "    4. Le ratio jeton/type\n",
    "    5. Le nombre total de lemmes distincts\n",
    "    6. Le nombre total de racines (stems) distinctes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "source": [
    "def total_non_distinct(corpus):\r\n",
    "    total = 0\r\n",
    "    for _i, paragraph in corpus.items():\r\n",
    "        total += len(paragraph)\r\n",
    "    return total\r\n",
    "    \r\n",
    "def total_distinct(corpus):\r\n",
    "    words = set()\r\n",
    "    for _i, paragraph in corpus.items():\r\n",
    "        for word in paragraph:\r\n",
    "            words.add(word)\r\n",
    "    return len(words)\r\n",
    "\r\n",
    "def most_frequent(corpus, n = 10):\r\n",
    "    words = {}\r\n",
    "    for _i, paragraph in corpus.items():\r\n",
    "        for word in paragraph:\r\n",
    "            if word in words:\r\n",
    "                words[word] += 1\r\n",
    "            else:\r\n",
    "                words[word] = 1\r\n",
    "    return list(sorted(words.items(), key=lambda temp: temp[1], reverse=True))[:n]\r\n",
    "    \r\n",
    "def ratio_jeton_type(corpus):\r\n",
    "    return total_non_distinct(corpus)/total_distinct(corpus)\r\n",
    "\r\n",
    "def total_lemme_distinct(corpus):\r\n",
    "    lemmed = corpus.apply(lambda words: [lem.lemmatize(word) for word in words])\r\n",
    "    return total_distinct(lemmed)\r\n",
    "    \r\n",
    "def total_stems_distinct(corpus):\r\n",
    "    stem = corpus.apply(lambda words: [ps.stem(word) for word in words])\r\n",
    "    return total_distinct(stem)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. (_1 point_) Ecrivez une fonction explore_corpus() qui fait appel à toutes les fonctions en 1) et imprime leur résultat.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "source": [
    "def explore_corpus(corpus):\r\n",
    "    print(\"nombre total de jetons: \",total_non_distinct(corpus))\r\n",
    "    print(\"Nombre total de mots distincts: \", total_distinct(corpus))\r\n",
    "    print(\"Mots les plus frequents: \",most_frequent(corpus))\r\n",
    "    print(\"Ratio jeton/type: \",ratio_jeton_type(corpus))\r\n",
    "    print(\"Nombre total lemmes distincts: \",total_lemme_distinct(corpus))\r\n",
    "    print(\"Nombre total de racines distincts: \",total_stems_distinct(corpus))\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. (_5 points_) Pour la suite du TP, vous devez effectuer le pré-traitement du corpus (questions, passages) en convertissant le texte en minuscules, en segmentant le texte, en supprimant les mots outils et en lemmatisant le texte. Chaque opération doit avoir sa fonction python si elle n’est pas déjà implantée dans la question 1) précédente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "source": [
    "corpus = pd.read_csv(PATH+\"corpus.csv\", usecols = [1])\r\n",
    "questions = pd.read_csv(PATH+\"train_ids.csv\", usecols = [1,2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "source": [
    "corpus.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           paragraph\n",
       "0  The Normans (Norman: Nourmands; French: Norman...\n",
       "1  The Norman dynasty had a major political, cult...\n",
       "2  The English name \"Normans\" comes from the Fren...\n",
       "3  In the course of the 10th century, the initial...\n",
       "4  Before Rollo's arrival, its populations did no..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Norman dynasty had a major political, cult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The English name \"Normans\" comes from the Fren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the course of the 10th century, the initial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Before Rollo's arrival, its populations did no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 461
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "source": [
    "questions.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                           question\n",
       "0   0  Who leaders the sub-divisions of offices or di...\n",
       "1   1  Besides using 3kV DC what other power type is ...\n",
       "2   2  How many other cities had populations larger t...\n",
       "3   3     Did von Neumann rule hidden variable theories?\n",
       "4   5  What is the name of the book that has the laws..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Who leaders the sub-divisions of offices or di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Besides using 3kV DC what other power type is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>How many other cities had populations larger t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Did von Neumann rule hidden variable theories?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What is the name of the book that has the laws...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 462
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "source": [
    "paragraphs = corpus[\"paragraph\"].apply(word_tokenize)\r\n",
    "paragraphs.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    [The, Normans, (, Norman, :, Nourmands, ;, Fre...\n",
       "1    [The, Norman, dynasty, had, a, major, politica...\n",
       "2    [The, English, name, ``, Normans, '', comes, f...\n",
       "3    [In, the, course, of, the, 10th, century, ,, t...\n",
       "4    [Before, Rollo, 's, arrival, ,, its, populatio...\n",
       "Name: paragraph, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 463
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "source": [
    "def transform_min(corpus):\r\n",
    "    return corpus.str.lower()\r\n",
    "\r\n",
    "def tokenize(corpus):\r\n",
    "    return corpus.apply(word_tokenize)\r\n",
    "\r\n",
    "def verify_word(word):\r\n",
    "    punctuation_stop_words = list(string.punctuation) + stopwords.words(\"english\")\r\n",
    "    # on retire les mots à 2 lettres et moins qui ne sont pas des chiffres\r\n",
    "    regex = re.compile(r\"^\\D{1,2}$\")\r\n",
    "    \r\n",
    "    if word not in punctuation_stop_words and regex.match(word) is None:\r\n",
    "        return True\r\n",
    "    return False\r\n",
    "\r\n",
    "def removeStopWords(paragraph):\r\n",
    "    lem = WordNetLemmatizer()\r\n",
    "    return [lem.lemmatize(word) for word in paragraph if verify_word(word)]\r\n",
    "\r\n",
    "\r\n",
    "def clean_data(paragraph):\r\n",
    "    paragraph_tokenized = word_tokenize(paragraph)\r\n",
    "    return removeStopWords(paragraph_tokenized)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "source": [
    "corpus_minuscule = transform_min(corpus[\"paragraph\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "source": [
    "corpus_cleaned = corpus_minuscule.apply(clean_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus_cleaned[1:5,]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    [norman, dynasty, major, political, cultural, ...\n",
       "2    [english, name, norman, come, french, word, no...\n",
       "3    [course, 10th, century, initially, destructive...\n",
       "4    [rollo, arrival, population, differ, picardy, ...\n",
       "Name: paragraph, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 223
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "explore_corpus(corpus_cleaned)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nombre total de jetons:  5669317\n",
      "Nombre total de mots distincts:  174784\n",
      "Mots les plus frequents:  [('/td', 66809), ('/tr', 30819), ('state', 27662), ('first', 27280), ('also', 22724), ('one', 20951), ('/th', 19643), ('year', 17680), ('new', 17358), ('united', 17093)]\n",
      "Ratio jeton/type:  32.43613259794947\n",
      "Nombre total lemmes distincts:  174764\n",
      "Nombre total de racines distincts:  152806\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "questions_minuscule = transform_min(questions[\"question\"])\r\n",
    "questions_cleaned = questions_minuscule.apply(clean_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "questions_cleaned.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0            [leader, sub-divisions, office, division]\n",
       "1    [besides, using, 3kv, power, type, used, forme...\n",
       "2       [many, city, population, larger, 40,000, 1500]\n",
       "3       [von, neumann, rule, hidden, variable, theory]\n",
       "4          [name, book, law, ethic, orthodox, judaism]\n",
       "Name: question, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2. Représentation de questions et de passages (14 points)\n",
    "\n",
    "1. (_10 points_) En utilisant sklearn et à partir de votre corpus pré-traité, vous devez implanter un modèle M1 qui est de représenter chaque passage et question avec votre vocabulaire, en utilisant un modèle sac de mots des n-grammes (n=1) qu’ils contiennent et en pondérant ces éléments avec TF-IDF. Notez que les questions doivent aussi être inclues dans la construction du vocabulaire."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "paragraphs_t = corpus_cleaned.apply(lambda x: \" \".join(x))\r\n",
    "questions_t = questions_cleaned.apply(lambda x: \" \".join(x))\r\n",
    "\r\n",
    "# On merge les paragraphs du corpus avec ceux des questions de notre training\r\n",
    "# Pour former le vocabulaire que nous utiliserons tout au long du tp\r\n",
    "questions_paragraph = pd.concat([paragraphs_t, questions_t], axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# On crée le model tfidf qui sera utiliser.\r\n",
    "def tfidf_model(model=1):\r\n",
    "  vectorizer = TfidfVectorizer(ngram_range=(1, model))\r\n",
    "  vectorizer.fit(questions_paragraph.values)\r\n",
    "  return vectorizer\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vectorizer_model1 = tfidf_model()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. (_4 points_) Expérimentez maintenant avec un modèle n-gramme (n=1,2) mélangeant les unigrammes et les bigrammes et pondéré avec TF-IDF."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vectorizer_model2 = tfidf_model(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour M1 et M2, assurez-vous de réutiliser la même fonction avec comme paramètre les n-grammes à considérer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3. Ordonnancement des passages (10 points)\n",
    "Maintenant que vous avez une représentation de vos passages et questions, il faut être capable de déterminer quel passage sera le plus pertinent pour la question posée. Il vous faut donc retrouver un top-N (N=1,5,10 … ) de passages utiles pour répondre à la question. Ces passages devront être ordonnés du plus pertinent au moins pertinent. Idéalement le passage à la position 1 sera celui qui contient la réponse à la question.\n",
    "<br>\n",
    "<br>\n",
    "Vous devez écrire des fonctions pour évaluer la similarité entre la représentation de la question et celle de chaque passage et retourner les N passage les plus similaires où N est un paramètre. \n",
    "1. (_5 points_) En utilisant la distance euclidienne\n",
    "2. (_5 points_) En utilisant la distance cosinus\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def distances_per_question(question, paragraphs,dist, n=1):\r\n",
    "  return np.argsort(dist(question, paragraphs)[0])[:n]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "questions_str_model1 = questions_cleaned.apply(lambda x: \" \".join(x))\r\n",
    "tfidf_questions_model1 = vectorizer_model1.transform(questions_str_model1)\r\n",
    "tfidf_questions_model2 = vectorizer_model2.transform(questions_str_model1)\r\n",
    "\r\n",
    "paragraphs_str_model1 = corpus_cleaned.apply(lambda x: \" \".join(x))\r\n",
    "tfidf_paragraphs_model1 = vectorizer_model1.transform(paragraphs_str_model1)\r\n",
    "tfidf_paragraphs_model2 = vectorizer_model2.transform(paragraphs_str_model1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Petit test pour verifier les resultats pour une des questions avec les deux distances."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "distances_per_question(tfidf_questions_model1[5,], tfidf_paragraphs_model1, cosine_distances, 50)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([42933, 28294,  5280, 67872, 11509,  4630, 13900, 11510, 66522,\n",
       "       53288, 12352, 13142,  5329,  5281, 77735, 79492, 15165, 55707,\n",
       "       55016, 65187, 53143, 49867, 56767,  3021, 31243, 24898, 81263,\n",
       "       30800, 79694, 35287,  1635, 65309,  4724,  2109, 38259,  8050,\n",
       "        2643, 78987, 65962, 13150, 20064, 56194, 15531,  8064, 24187,\n",
       "        4375, 35491,  8086, 15509, 72536], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 378
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "distances_per_question(tfidf_questions_model1[5,], tfidf_paragraphs_model1, euclidean_distances, 50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4. Évaluation (15 points)\n",
    "En utilisant votre ensemble de validation : <br>\n",
    "1. (_5 points_) Vous devez calculer la précision top-N (N=1,5,10, 50) de votre modèle M1 et M2 avec la distance euclidienne et cosinus et les afficher. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "validation_data = pd.read_csv(PATH+\"val_ids.csv\", usecols = [1,2,3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "questions_minuscule_valid = transform_min(validation_data[\"question\"])\r\n",
    "questions_cleaned_valid = questions_minuscule_valid.apply(clean_data)\r\n",
    "\r\n",
    "questions_valid_str = questions_cleaned_valid.apply(lambda x: \" \".join(x))\r\n",
    "\r\n",
    "tfidf_questions_valid_model1 = vectorizer_model1.transform(questions_valid_str)\r\n",
    "tfidf_questions_valid_model2 = vectorizer_model2.transform(questions_valid_str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def precision(answers, questions, paragraphs, dist):\r\n",
    "  nbr = {\"1\":0, \"5\":0, \"10\": 0, \"50\": 0}\r\n",
    "  for index, answer in answers.items():\r\n",
    "    all_distances_for_row = distances_per_question(questions[index], paragraphs, dist, 50)\r\n",
    "    if answer == all_distances_for_row[0]:\r\n",
    "      nbr[\"1\"] += 1\r\n",
    "      nbr[\"5\"] += 1\r\n",
    "      nbr[\"10\"] += 1\r\n",
    "      nbr[\"50\"] += 1\r\n",
    "    elif answer in all_distances_for_row[1:5]:\r\n",
    "      nbr[\"5\"] += 1\r\n",
    "      nbr[\"10\"] += 1\r\n",
    "      nbr[\"50\"] += 1\r\n",
    "    elif answer in all_distances_for_row[6:10]:\r\n",
    "      nbr[\"50\"] += 1\r\n",
    "      nbr[\"10\"] += 1\r\n",
    "    elif answer in all_distances_for_row[11:50]:\r\n",
    "      nbr[\"50\"] += 1\r\n",
    "          \r\n",
    "  return nbr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "validation_data_size = validation_data.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# M1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distance cosinus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "m1c = precision(validation_data[\"paragraph_id\"],tfidf_questions_valid_model1, tfidf_paragraphs_model1, cosine_distances)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"m1c1\", m1c[\"1\"]/validation_data_size)\r\n",
    "print(\"m1c5\", m1c[\"5\"]/validation_data_size)\r\n",
    "print(\"m1c10\", m1c[\"10\"]/validation_data_size)\r\n",
    "print(\"m1c50\", m1c[\"10\"]/validation_data_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "m1c1 0.25871559633027524\n",
      "m1c5 0.4710703363914373\n",
      "m1c10 0.5387155963302752\n",
      "m1c50 0.5387155963302752\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distance euclidienne"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "m1e = precision(validation_data[\"paragraph_id\"],tfidf_questions_valid_model1, tfidf_paragraphs_model1, euclidean_distances)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"m1e1\", m1e[\"1\"]/validation_data_size)\r\n",
    "print(\"m1e5\", m1e[\"5\"]/validation_data_size)\r\n",
    "print(\"m1e10\", m1e[\"10\"]/validation_data_size)\r\n",
    "print(\"m1e50\", m1e[\"10\"]/validation_data_size)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "m1e1 0.25871559633027524\n",
      "m1e5 0.47021406727828746\n",
      "m1e10 0.5388379204892967\n",
      "m1e50 0.5388379204892967\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# M2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distance cosinus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "m2c = precision(validation_data[\"paragraph_id\"],tfidf_questions_valid_model2, tfidf_paragraphs_model2, cosine_distances)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"m2c1\", m2c[\"1\"]/validation_data_size)\r\n",
    "print(\"m2c5\", m2c[\"5\"]/validation_data_size)\r\n",
    "print(\"m2c10\", m2c[\"10\"]/validation_data_size)\r\n",
    "print(\"m2c50\", m2c[\"10\"]/validation_data_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "m2c1 0.3201223241590214\n",
      "m2c5 0.5351681957186545\n",
      "m2c10 0.6053822629969419\n",
      "m2c50 0.6053822629969419\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distance euclidienne"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "m2e = precision(validation_data[\"paragraph_id\"],tfidf_questions_valid_model2, tfidf_paragraphs_model2, euclidean_distances)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"m2e1\", m2e[\"1\"]/validation_data_size)\r\n",
    "print(\"m2e5\", m2e[\"5\"]/validation_data_size)\r\n",
    "print(\"m2e10\", m2e[\"10\"]/validation_data_size)\r\n",
    "print(\"m2e50\", m2e[\"10\"]/validation_data_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "m2e1 0.3180428134556575\n",
      "m2e5 0.5360244648318043\n",
      "m2e10 0.6077064220183486\n",
      "m2e50 0.6077064220183486\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. (_5 points_) Pour chacun de ces modèles, générez une courbe de performance faisant varier le N (N=1, 5, 10, 50)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "N = [1,5,10,50]\r\n",
    "plt.xlabel(\"Nombre de reponses considere\")\r\n",
    "plt.ylabel(\"precision\")\r\n",
    "plt.plot(N, [ e/validation_data_size for e in m1e.values()], color=\"blue\", label=\"model 1\", markersize=12)\r\n",
    "plt.plot(N, [ e/validation_data_size for e in m2e.values()], color=\"green\", label=\"model 2\", markersize=12)\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 385.78125 262.19625\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-09-23T00:23:22.983414</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 385.78125 262.19625 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\nL 378.58125 7.2 \r\nL 43.78125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m0a0468f7d9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.787929\" xlink:href=\"#m0a0468f7d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(49.606679 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"114.902957\" xlink:href=\"#m0a0468f7d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(108.540457 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"177.017985\" xlink:href=\"#m0a0468f7d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(170.655485 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.133013\" xlink:href=\"#m0a0468f7d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(232.770513 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"301.24804\" xlink:href=\"#m0a0468f7d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(294.88554 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.363068\" xlink:href=\"#m0a0468f7d9\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(357.000568 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Nombre de reponses considere -->\r\n     <g transform=\"translate(133.35 252.916563)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.8125 72.90625 \r\nL 23.09375 72.90625 \r\nL 55.421875 11.921875 \r\nL 55.421875 72.90625 \r\nL 64.984375 72.90625 \r\nL 64.984375 0 \r\nL 51.703125 0 \r\nL 19.390625 60.984375 \r\nL 19.390625 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-78\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n       <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path id=\"DejaVuSans-32\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-78\"/>\r\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"135.986328\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"233.398438\" xlink:href=\"#DejaVuSans-98\"/>\r\n      <use x=\"296.875\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"335.738281\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"397.261719\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"429.048828\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"492.525391\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"554.048828\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"585.835938\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"624.699219\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"686.222656\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"749.699219\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"810.880859\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"874.259766\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"926.359375\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"987.882812\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"1039.982422\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"1071.769531\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"1126.75\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"1187.931641\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"1251.310547\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"1303.410156\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"1331.193359\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"1394.669922\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"1456.193359\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"1495.056641\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m6f37bd139e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m6f37bd139e\" y=\"198.64952\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.3 -->\r\n      <g transform=\"translate(20.878125 202.448739)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m6f37bd139e\" y=\"159.635166\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(20.878125 163.434385)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m6f37bd139e\" y=\"120.620812\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(20.878125 124.420031)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m6f37bd139e\" y=\"81.606458\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(20.878125 85.405677)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m6f37bd139e\" y=\"42.592104\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.7 -->\r\n      <g transform=\"translate(20.878125 46.391323)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_13\">\r\n     <!-- precision -->\r\n     <g transform=\"translate(14.798438 138.473906)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"102.339844\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"163.863281\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"218.84375\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"246.626953\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"298.726562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"326.509766\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"387.691406\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#p137032f99d)\" d=\"M 58.999432 214.756364 \r\nL 83.845443 132.241601 \r\nL 114.902957 105.468448 \r\nL 363.363068 32.212138 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p137032f99d)\" d=\"M 58.999432 191.610233 \r\nL 83.845443 106.5661 \r\nL 114.902957 78.599847 \r\nL 363.363068 17.083636 \r\n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 43.78125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 224.64 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 224.64 \r\nL 378.58125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 7.2 \r\nL 378.58125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 50.78125 44.55625 \r\nL 123.460938 44.55625 \r\nQ 125.460938 44.55625 125.460938 42.55625 \r\nL 125.460938 14.2 \r\nQ 125.460938 12.2 123.460938 12.2 \r\nL 50.78125 12.2 \r\nQ 48.78125 12.2 48.78125 14.2 \r\nL 48.78125 42.55625 \r\nQ 48.78125 44.55625 50.78125 44.55625 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_14\">\r\n     <path d=\"M 52.78125 20.298438 \r\nL 72.78125 20.298438 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\"/>\r\n    <g id=\"text_14\">\r\n     <!-- model 1 -->\r\n     <g transform=\"translate(80.78125 23.798438)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"311.376953\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"343.164062\" xlink:href=\"#DejaVuSans-49\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_16\">\r\n     <path d=\"M 52.78125 34.976562 \r\nL 72.78125 34.976562 \r\n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\"/>\r\n    <g id=\"text_15\">\r\n     <!-- model 2 -->\r\n     <g transform=\"translate(80.78125 38.476562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"311.376953\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"343.164062\" xlink:href=\"#DejaVuSans-50\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p137032f99d\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. (_5 points_) A cette étape, vous devez produire un fichier _passage_submission_M1.csv_ et _passage_submission_M2.csv_ qui contient pour toutes les questions de l’ensemble de test le top-N des passages retournés par votre modèle M1 et M2 pour y répondre. C’est à vous de déterminer si vous utiliserez la distance euclidienne ou cosinus basé sur vos résultats d’évaluation sur l’ensemble de validation en 1) et 2). Le fichier doit respecter le format suivant pour chaque top_N(N=1,5,10,50) :  <QuestionID, PassageID1 ;… ;PassageIDN>. Le format est démontré dans _sample_passage_submission.csv_."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_data = pd.read_csv(PATH+\"test.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "questions_minuscule_test = transform_min(test_data[\"question\"])\r\n",
    "questions_cleaned_test = questions_minuscule_test.apply(clean_data)\r\n",
    "\r\n",
    "questions_test_str = questions_cleaned_test.apply(lambda x: \" \".join(x))\r\n",
    "\r\n",
    "tfidf_questions_test_model1 = vectorizer_model1.transform(questions_test_str)\r\n",
    "tfidf_questions_test_model2 = vectorizer_model2.transform(questions_test_str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def top_n(ids, questions, paragraphs, dist, n):\r\n",
    "  tops = []\r\n",
    "  for index, id in ids.items():\r\n",
    "    all_distances_for_row = distances_per_question(questions[index], paragraphs, dist, 50)\r\n",
    "    top = {}\r\n",
    "    top[\"id\"] = id\r\n",
    "    top[\"1\"] = all_distances_for_row[0]\r\n",
    "    top[\"5\"] = \";\".join(str(e) for e in all_distances_for_row[0:5])\r\n",
    "    top[\"10\"] = \";\".join(str(e) for e in all_distances_for_row[0:10])\r\n",
    "    top[\"n\"] = \";\".join(str(e) for e in all_distances_for_row[0:n])\r\n",
    "    tops.append(top)\r\n",
    "  return tops"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_n_m1 = top_n(test_data[\"id\"],tfidf_questions_test_model1, tfidf_paragraphs_model1, euclidean_distances, 50)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-435-cedd22e11c6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtop_n_m1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf_questions_test_model1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_paragraphs_model1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-432-1b29c4c6691c>\u001b[0m in \u001b[0;36mtop_n\u001b[1;34m(ids, questions, paragraphs, dist, n)\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[0mtops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mall_distances_for_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistances_per_question\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparagraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-364-185a5f94ae7d>\u001b[0m in \u001b[0;36mdistances_per_question\u001b[1;34m(question, paragraphs, dist, n)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdistances_per_question\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparagraphs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparagraphs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[1;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    558\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[0;32m    559\u001b[0m                              \"use '*' instead\")\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mmajor_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# convert to this format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         idx_dtype = get_index_dtype((self.indptr, self.indices,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36masformat\u001b[1;34m(self, format, copy)\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[1;31m# Forward the copy kwarg, if it's accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\csc.py\u001b[0m in \u001b[0;36mtocsr\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         csc_tocsr(M, N,\n\u001b[0m\u001b[0;32m    143\u001b[0m                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1 = pd.DataFrame(top_n_m1)\r\n",
    "df1.to_csv(\"passage_submission_M1.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_n_m2 = precision(test_data[\"id\"],tfidf_questions_test_model2, tfidf_paragraphs_model2, euclidean_distances, 50)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2 = pd.DataFrame(top_n_m2)\r\n",
    "df2.to_csv(\"passage_submission_M2.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.5. Le plus (24 points)\n",
    "\n",
    "1. (_21 points_) Vous devez proposer un modèle M3 différent (basé sur l’apprentissage machine par exemple) afin de déterminer un score de pertinence d’un passage pour une question donnée et ordonner les passages. \n",
    "    - Faites une petite recherche sur l’état de l’art en consultant https://nlp.stanford.edu/IR-book/information-retrieval-book.html\n",
    "    - Vous êtes libres de proposer une autre métrique de poids, ou une autre façon d’ordonner les passages (exemple : méthodes de type _learning to rank_) et de partir de votre corpus initial ou de votre ordonnancement en M1/M2 (choisissez le meilleur) et de réordonnancer les passages obtenus par votre premier modèle.\n",
    "    - Expliquez votre modèle et son intérêt dans votre notebook. Le nombre de points obtenus dépendra de l’effort mis dans cette partie."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Explication de ce que allons faire\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "Nous allons utiliser plusieurs modèles de langage naturel connus pour répondre à l'objectif de pouvoir associé le bon extrait de paragraphe à la bonne question. De plus, nous allons expliquer comment est-ce que les différents modèles fonctionnent.\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "Les différents modèles que nous allons explorer sont les suivants :\r\n",
    "- word2vec\r\n",
    "- fastText\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "### Traitement des données : \r\n",
    "\r\n",
    "<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gensim\r\n",
    "\r\n",
    "corpus_paragraphs = pd.read_csv(PATH + \"corpus.csv\")[\"paragraph\"]\r\n",
    "corpus_cleaned = clean_data(corpus_paragraphs)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#training data processing\r\n",
    "train_df = pd.read_csv(PATH + \"train_ids.csv\")\r\n",
    "train_questions = train_df[\"question\"]\r\n",
    "train_paragraphs = train_df[\"paragraph_id\"].apply(lambda x : corpus_paragraphs[x])\r\n",
    "\r\n",
    "train_cleaned_questions = clean_data(train_questions)\r\n",
    "train_cleaned_paragraphs = clean_data(train_paragraphs)\r\n",
    "\r\n",
    "#validation data processing\r\n",
    "val_df = pd.read_csv(PATH + \"val_ids.csv\")\r\n",
    "val_questions = val_df[\"question\"]\r\n",
    "val_cleaned_questions = clean_data(val_questions)\r\n",
    "\r\n",
    "corpus_cleaned_unique = clean_data(pd.Series(corpus_paragraphs.unique()))\r\n",
    "all_vocab = corpus_cleaned_unique.to_list() + train_cleaned_questions.to_list() + val_cleaned_questions.to_list()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fonctions communes :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def vectorize(sentence, model):\r\n",
    "    ans = []\r\n",
    "    for word in sentence:\r\n",
    "        if model.wv.has_index_for(word):\r\n",
    "            ans.append(model.wv[word])\r\n",
    "    if(len(ans) == 0):\r\n",
    "        return np.array([np.zeros(200)])\r\n",
    "    return np.array(ans)\r\n",
    "\r\n",
    "def vectorize_sentence(processed_data, model):\r\n",
    "    vec_data = processed_data.apply(lambda sentence : vectorize(sentence, model))\r\n",
    "    vec_data_sentence = vec_data.apply(lambda x : np.mean(x ,axis = 0) if len(x) != 1 else x[0])\r\n",
    "    return np.stack(vec_data_sentence.values)\r\n",
    "\r\n",
    "    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### word2vec\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "##### L'intuition derrière word2vec\r\n",
    "\r\n",
    "Un word2vec est un modèle d'apprentissage machine qui possède 2 couches de neuronnes. Ce modèle va essayer d'apprendre comment construire un espace vectoriel où chaque mot qui est définie dans le vocabulaire du modèle est représenté comme un vecteur. Des mots tels que chats et chien devraient, donc avoir une distance entre eux assez basse vu que ce sont des mots qui se ressemble relativement beaucoup (ces mots représente des animaux domestiques qui sont à peu près de même taille). \r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "Le modèle apprends à faire le plongement linguistique en mettant en relation les différents mots avec leur contexte. Ceci veut dire qu'un mot qui partage souvent un contexte similaire à un autre va se trouver à être d'une signification similaire pour le modèle. C'est important de comprendre cela, car cela vient expliquer pourquoi, pour qu'un mot soit compris par le modèle, il faut réellement qu'il puisse avoir été vu lors de la phase d'entrainement. Cela est du au fait que sinon le mot aura de la misère à se trouver une place dans l'espace vectoriel.\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "##### Comment ça fonctionne mathématiquement ?\r\n",
    "\r\n",
    "Dans le contexte de cette explication, nous allons expliquer que comment CBOW fonctionne vu que Skip-Gram n'a pas été utilisé dans le code. De plus, je ne vais pas rentrer extrêmement dans le détail de comment le modèle fait pour apprendre, mais je vais, toutefois, vous donner une idée générale de la procédure à suivre pour l'implémenter. \r\n",
    "\r\n",
    "Le modèle CBOW (acronyme pour \"Continuous Bag or Words\") va apprendre la valeur vectorizé de chaque mot en essayant de prédire un mot lorsqu'on lui donne le contexte qui entoure ce mot. Pour ce faire, nous allons attribuer aux différents mots un vecteur aléatoire et, ensuite, faire de l'apprentissage supervisé se basant sur une descente en gradient pour ajuster les poids du réseaux de neuronnes. \r\n",
    "Voici certaines informations en ce qui attrait à l'implémentation :\r\n",
    "\r\n",
    "- Loss Function : Logarithmic loss\r\n",
    "- Activation Function : Softmax\r\n",
    "- Backward Propagation Formulas can be found at this link : https://srishtee-kriti.medium.com/mathematics-behind-continuous-bag-of-words-cbow-model-1e54cc2ecd88 \r\n",
    "\r\n",
    "<br>\r\n",
    "<h3><center>Archictecture CBOW</center></h3>\r\n",
    "\r\n",
    "![image.png](https://miro.medium.com/max/604/1*DfuBd49nCtT99h328iXL2Q.png)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.models import Word2Vec\r\n",
    "\r\n",
    "#train the word2vec model\r\n",
    "word2vec_model = Word2Vec(all_vocab, vector_size = 200)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#vectorize the data\r\n",
    "print(\"Starting the vectorization of the data word2vec\")\r\n",
    "train_vec_paragraphs_sentence = vectorize_sentence(train_cleaned_paragraphs, word2vec_model)\r\n",
    "train_vec_questions_sentence = vectorize_sentence(train_cleaned_questions, word2vec_model)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# NE PAS RUN -- PRENDS BEAUCOUP DE TEMPS À APPRENDRE -- RUN LA PROCHAINE POUR RUN CELUI QUI FUT ENTRAINÉ\r\n",
    "\r\n",
    "from sklearn.multioutput import MultiOutputRegressor\r\n",
    "from sklearn.svm import SVR\r\n",
    "from joblib import dump\r\n",
    "\r\n",
    "regressor = SVR(gamma='auto')\r\n",
    "SVR_word2vec = MultiOutputRegressor(regressor, n_jobs = 1)\r\n",
    "SVR_word2vec.fit(train_vec_questions_sentence[:10000, ], train_vec_paragraphs_sentence[:10000, ])\r\n",
    "dump(SVR_word2vec, 'SVR_word2vec_trained.joblib')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from joblib import load\r\n",
    "SVR_word2vec = load('SVR_word2vec_trained.joblib')\r\n",
    "\r\n",
    "#validation  \r\n",
    "val_vec_questions_sentence_word2vec = vectorize_sentence(val_cleaned_questions, word2vec_model)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FastText\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "Tout comme Word2Vec, l'algorithme fasttext va essayer de trouver une représentation vectorielle des mots dans le vocabulaire. Toutefois, la manière dont il va le faire est complètement différente. FastText va faire un n-gramme de lettres pour donner du sens au différents mots qui se trouve dans le vocabulaire. \r\n",
    "\r\n",
    "<br>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.models import FastText\r\n",
    "\r\n",
    "fasttext_model = FastText(all_vocab, vector_size = 200, window=5)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#vectorize the data\r\n",
    "print(\"Starting the vectorization of the data fasttext\")\r\n",
    "train_vec_paragraphs_fasttext = vectorize_sentence(train_cleaned_paragraphs, fasttext_model)\r\n",
    "train_vec_questions_fasttext = vectorize_sentence(train_cleaned_questions, fasttext_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\r\n",
    "from sklearn.svm import SVR\r\n",
    "from joblib import dump\r\n",
    "\r\n",
    "regressor = SVR(gamma='auto')\r\n",
    "SVR_fasttext = MultiOutputRegressor(regressor, n_jobs = 1)\r\n",
    "SVR_fasttext.fit(train_vec_questions_fasttext[:10000, ], train_vec_paragraphs_fasttext[:10000, ])\r\n",
    "dump(SVR_fasttext, 'SVR_fasttext_trained.joblib')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from joblib import load\r\n",
    "SVR_word2vec = load('SVR_fasttext_trained.joblib')\r\n",
    "\r\n",
    "#validation  \r\n",
    "val_vec_questions_sentence_fasttext = vectorize_sentence(val_cleaned_questions, fasttext_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Liens de références\r\n",
    "\r\n",
    "1. https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469\r\n",
    "2. https://israelg99.github.io/2017-03-23-Word2Vec-Explained/\r\n",
    "3. https://kavita-ganesan.com/fasttext-vs-word2vec/#.YUf3C7hKiUk\r\n",
    "4. https://srishtee-kriti.medium.com/mathematics-behind-continuous-bag-of-words-cbow-model-1e54cc2ecd88\r\n",
    "5. https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3\r\n",
    "6. https://medium.com/analytics-vidhya/how-i-build-a-question-answering-model-3548878d5db2\r\n",
    "\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. (_2  point_) Vous devez ensuite afficher l’évaluation de votre modèle M3 tel que décrit dans la section 5.4 Evaluation en utilisant les mêmes fonctions. Notamment, vous devez comparer les performances de vos modèles M1, M2 et M3 sur l’ensemble de validation avec une courbe de performance faisant varier le N (N=1, 5, 10, …)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def precision(data, vec_questions, vec_paragraphs, clf, n = 1, dist = euclidean_distances):\r\n",
    "  nbr = 0\r\n",
    "  for index, row in data.iterrows():\r\n",
    "    predicted_paragraph = clf.predict(vec_questions[index].reshape(1, -1))\r\n",
    "    predictions = distances_per_question(predicted_paragraph, vec_paragraphs,euclidean_distances, n)\r\n",
    "    if row[\"paragraph_id\"] in predictions:\r\n",
    "      nbr += 1\r\n",
    "  return nbr / data.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus_vec_paragraphs_sentence_word2vec = vectorize_sentence(corpus_cleaned, word2vec_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# word2vec eucledian\r\n",
    "print(\"Started word2vec eucledian\")\r\n",
    "ns = [50]\r\n",
    "for n in ns:\r\n",
    "    print(precision(val_df, val_vec_questions_sentence_word2vec, corpus_vec_paragraphs_sentence_word2vec, SVR_word2vec, n = n))\r\n",
    "print(\"Done !\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# word2vec cosine similarity\r\n",
    "print(\"Started word2vec cosine similarity\")\r\n",
    "ns = [50]\r\n",
    "for n in ns:\r\n",
    "    print(precision(val_df[:100], val_vec_questions_sentence_word2vec[:100], corpus_vec_paragraphs_sentence_word2vec, SVR_word2vec, n = n, dist = cosinus_distances_per_question))\r\n",
    "    \r\n",
    "print(\"Done !\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus_vec_paragraphs_sentence_fasttext = vectorize_sentence(corpus_cleaned, fasttext_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fasttext eucledian\r\n",
    "print(\"Started fasttext eucledian\")\r\n",
    "ns = [50]\r\n",
    "for n in ns:\r\n",
    "    print(precision(val_df[:100], val_vec_questions_sentence_fasttext[:100], corpus_vec_paragraphs_sentence_fasttext, SVR_fasttext, n = n))\r\n",
    "print(\"Done !\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fasttext cosine similarity\r\n",
    "print(\"Started fasttext eucledian\")\r\n",
    "ns = [50]\r\n",
    "for n in ns:\r\n",
    "    print(precision(val_df[:100], val_vec_questions_sentence_fasttext[:100], corpus_vec_paragraphs_sentence_fasttext, SVR_fasttext, n = n, dist = cosinus_distances_per_question))\r\n",
    "print(\"Done !\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explication des résultats\r\n",
    "\r\n",
    "Les résultats pourraient sembler relativement mauvais comparément aux résultats au TF-IDF, mais le SVR c'est fait entrainer avec seulement 1/8 des données étant donné que le temps d'entrainement prennait beaucoup de temps. Ceci explique pourquoi est-ce que les résultats sont plus bas. Prenant ceci en considération, les résultats sont assez impressionnants.\r\n",
    "\r\n",
    "<br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. (_1 point_) En utilisant votre modèle M3, vous devez produire un fichier passage_submission_M3.csv qui contient pour toutes les questions de l’ensemble de test le top-N des passages retournés par votre système pour y répondre. Le fichier doit respecter le format suivant pour chaque top_N (N=1,5,10,50) :  <QuestionID, PassageID1…PassageIDN>. _Le format est démontré dans sample_passage_submission.csv_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_data = pd.read_csv(PATH+'test.csv')\r\n",
    "test_cleaned = clean_data(test_data[\"question\"])\r\n",
    "test_vec = vectorize_sentence(test_cleaned, word2vec_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"starting\")\r\n",
    "ns = [1, 5, 10]\r\n",
    "\r\n",
    "ans = []\r\n",
    "print(test_data.shape)\r\n",
    "for index, row in test_data.iterrows():\r\n",
    "    predicted_paragraph = SVR_word2vec.predict(test_vec[index].reshape(1, -1))\r\n",
    "    predictions = distances_per_question(predicted_paragraph, corpus_vec_paragraphs_sentence_word2vec,euclidean_distances, ns[-1])\r\n",
    "    str_predictions = list(map(str, predictions))\r\n",
    "    ans.append([';'.join(str_predictions[:n]) for n in ns])\r\n",
    "    \r\n",
    "print(\"Starting the save !\")\r\n",
    "pd.DataFrame(ans).to_csv('submission.csv', header = [\"top_{}\".format(n) for n in ns], index_label = \"id\")\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LIVRABLES\n",
    "Vous devez remettre sur Moodle:\n",
    "1. _Le code_ : Un Jupyter notebook en Python qui contient le code implanté avec les librairies permises. Le code doit être exécutable sans erreur et accompagné des commentaires appropriés dans le notebook de manière à expliquer les différentes fonctions et étapes dans votre projet. Nous nous réservons le droit de demander une démonstration ou la preuve que vous avez effectué vous-mêmes les expériences décrites. _Attention, en aucun cas votre code ne doit avoir été copié d’une quelconque source_. Les critères de qualité tels que la lisibilité du code et des commentaires sont importants. Tout votre code et vos résultats doivent être exécutables et reproductibles ; \n",
    "2. Un fichier _requirements.txt_ doit indiquer toutes les librairies / données nécessaires ;\n",
    "3. Un lien _GoogleDrive_ ou similaire vers les modèles nécessaires pour exécuter votre notebook si approprié ;\n",
    "4. Les fichiers de soumission de données de test _passage_submission_M1.csv_ et _passage_submission_M2.csv_\n",
    "5. Un document _contributions.txt_ : Décrivez brièvement la contribution de chaque membre de l’équipe. Tous les membres sont censés contribuer au développement. Bien que chaque membre puisse effectuer différentes tâches, vous devez vous efforcer d’obtenir une répartition égale du travail. En particulier, tous les membres du projet devraient participer à la conception du TP et participer activement à la réflexion et à l’implémentation du code.\n",
    "\n",
    "## EVALUATION \n",
    "Votre TP sera évalué selon les critères suivants :\n",
    "1. Exécution correcte du code\n",
    "2. Performance correcte des modèles\n",
    "3. Organisation du notebook\n",
    "4. Qualité du code (noms significatifs, structure, performance, gestion d’exception, etc.)\n",
    "5. Commentaires clairs et informatifs\n",
    "\n",
    "## CODE D’HONNEUR\n",
    "- Règle 1:  Le plagiat de code est bien évidemment interdit.\n",
    "- Règle 2: Vous êtes libres de discuter des idées et des détails de mise en œuvre avec d'autres équipes. Cependant, vous ne pouvez en aucun cas consulter le code d'une autre équipe INF8460, ou incorporer leur code dans votre TP.\n",
    "- Règle 3:  Vous ne pouvez pas partager votre code publiquement (par exemple, dans un dépôt GitHub public) tant que le cours n'est pas fini.\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}