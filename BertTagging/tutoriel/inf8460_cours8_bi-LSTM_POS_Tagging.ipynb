{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpPfQrczyoZl"
   },
   "source": [
    "# Bi-LSTM pour l'Ã©tiquetage morpho-syntaxique\n",
    "Application du tutoriel https://nlpforhackers.io/lstm-pos-tagger-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UV5X-wxdv4n0",
    "outputId": "bb80492a-68a6-4fff-c58e-3d7ac291d406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to C:\\Users\\Amal\n",
      "[nltk_data]     Zouaq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentences:  3914\n",
      "Tagged words: 100676\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"treebank\")\n",
    " \n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    " \n",
    "print(tagged_sentences[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LyTHD0sPwVF8",
    "outputId": "4ce96824-0838-4895-8100-c40f353fae20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
      " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
      " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
      " '.']\n",
      "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
      " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
      " '.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "sentences, sentence_tags =[], [] \n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))\n",
    " \n",
    "# Let's see how a sequence looks\n",
    " \n",
    "print(sentences[5])\n",
    "print(sentence_tags[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "A1i4llJHwY29"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    " \n",
    "(train_sentences, \n",
    " test_sentences, \n",
    " train_tags, \n",
    " test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f720hcZCwcgk"
   },
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    " \n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    " \n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XbuZ1j8whg9",
    "outputId": "2af8937b-6111-48cd-8041-32b9031d8e8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7013, 9180, 1812, 6106, 211, 2935, 6694, 476, 3112, 7935, 8961, 5172, 4156, 932, 781]\n",
      "[724, 704, 716, 5821, 9777, 6246, 2171, 334, 9328, 1655, 7787, 2122, 257, 3660, 1812, 5393, 9315, 9816, 4703, 2974, 4545, 9092, 8018, 7696, 6329, 4703, 9229, 2935, 4415, 3086, 1812, 3706, 4932, 4156, 2530, 353, 781]\n",
      "[45, 15, 45, 26, 26, 2, 26, 26, 7, 26, 26, 19, 34, 15, 29]\n",
      "[32, 42, 26, 26, 7, 26, 26, 44, 26, 6, 19, 13, 3, 19, 45, 37, 37, 15, 42, 37, 15, 32, 42, 45, 15, 42, 12, 2, 13, 23, 45, 32, 18, 34, 37, 15, 29]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    " \n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    " \n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfhTYNNDwpDD",
    "outputId": "d2e006e9-735f-486e-f321-aed3fb4d7409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hdNld_qwuaj",
    "outputId": "0d3c2a0e-2216-4d42-b89b-bd3cad833981"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7013 9180 1812 6106  211 2935 6694  476 3112 7935 8961 5172 4156  932\n",
      "  781    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[ 724  704  716 5821 9777 6246 2171  334 9328 1655 7787 2122  257 3660\n",
      " 1812 5393 9315 9816 4703 2974 4545 9092 8018 7696 6329 4703 9229 2935\n",
      " 4415 3086 1812 3706 4932 4156 2530  353  781    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n",
      "[45 15 45 26 26  2 26 26  7 26 26 19 34 15 29  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n",
      "[32 42 26 26  7 26 26 44 26  6 19 13  3 19 45 37 37 15 42 37 15 32 42 45\n",
      " 15 42 12  2 13 23 45 32 18 34 37 15 29  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWiaFgFpw6Dv",
    "outputId": "c84b81f9-7d38-4cd3-9b45-52dd7378f3ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 271, 128)          1290624   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 271, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 271, 47)           24111     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 2,103,215\n",
      "Trainable params: 2,103,215\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JqN1L4VQw-Js"
   },
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CulQ-41Jw_Cb",
    "outputId": "668cc52f-8295-4782-b50e-3ccfa420bbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print(cat_train_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5q7fs811xBgq",
    "outputId": "8376c152-4055-4ff7-d663-7d42cc4f8dcb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amal Zouaq\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2504 samples, validate on 627 samples\n",
      "Epoch 1/40\n",
      "2504/2504 [==============================] - 142s 57ms/step - loss: 1.2233 - accuracy: 0.8588 - val_loss: 0.3586 - val_accuracy: 0.9086\n",
      "Epoch 2/40\n",
      "2504/2504 [==============================] - 215s 86ms/step - loss: 0.3329 - accuracy: 0.9087 - val_loss: 0.3098 - val_accuracy: 0.9114\n",
      "Epoch 3/40\n",
      "2504/2504 [==============================] - 259s 103ms/step - loss: 0.3102 - accuracy: 0.9138 - val_loss: 0.2978 - val_accuracy: 0.9191\n",
      "Epoch 4/40\n",
      "2504/2504 [==============================] - 269s 108ms/step - loss: 0.2990 - accuracy: 0.9173 - val_loss: 0.2888 - val_accuracy: 0.9193\n",
      "Epoch 5/40\n",
      "2504/2504 [==============================] - 281s 112ms/step - loss: 0.2902 - accuracy: 0.9174 - val_loss: 0.2803 - val_accuracy: 0.9200\n",
      "Epoch 6/40\n",
      "2504/2504 [==============================] - 288s 115ms/step - loss: 0.2820 - accuracy: 0.9189 - val_loss: 0.2727 - val_accuracy: 0.9226\n",
      "Epoch 7/40\n",
      "2504/2504 [==============================] - 285s 114ms/step - loss: 0.2753 - accuracy: 0.9219 - val_loss: 0.2671 - val_accuracy: 0.9253\n",
      "Epoch 8/40\n",
      "2504/2504 [==============================] - 310s 124ms/step - loss: 0.2699 - accuracy: 0.9236 - val_loss: 0.2618 - val_accuracy: 0.9264\n",
      "Epoch 9/40\n",
      "2504/2504 [==============================] - 290s 116ms/step - loss: 0.2648 - accuracy: 0.9255 - val_loss: 0.2552 - val_accuracy: 0.9299\n",
      "Epoch 10/40\n",
      "2504/2504 [==============================] - 286s 114ms/step - loss: 0.2564 - accuracy: 0.9337 - val_loss: 0.2452 - val_accuracy: 0.9388\n",
      "Epoch 11/40\n",
      "2504/2504 [==============================] - 290s 116ms/step - loss: 0.2426 - accuracy: 0.9423 - val_loss: 0.2285 - val_accuracy: 0.9457\n",
      "Epoch 12/40\n",
      "2504/2504 [==============================] - 292s 116ms/step - loss: 0.2197 - accuracy: 0.9472 - val_loss: 0.2020 - val_accuracy: 0.9495\n",
      "Epoch 13/40\n",
      "2504/2504 [==============================] - 290s 116ms/step - loss: 0.1885 - accuracy: 0.9519 - val_loss: 0.1710 - val_accuracy: 0.9550\n",
      "Epoch 14/40\n",
      "2504/2504 [==============================] - 294s 117ms/step - loss: 0.1560 - accuracy: 0.9598 - val_loss: 0.1425 - val_accuracy: 0.9626\n",
      "Epoch 15/40\n",
      "2504/2504 [==============================] - 290s 116ms/step - loss: 0.1280 - accuracy: 0.9670 - val_loss: 0.1196 - val_accuracy: 0.9684\n",
      "Epoch 16/40\n",
      "2504/2504 [==============================] - 287s 115ms/step - loss: 0.1050 - accuracy: 0.9740 - val_loss: 0.1009 - val_accuracy: 0.9753\n",
      "Epoch 17/40\n",
      "2504/2504 [==============================] - 291s 116ms/step - loss: 0.0862 - accuracy: 0.9806 - val_loss: 0.0858 - val_accuracy: 0.9797\n",
      "Epoch 18/40\n",
      "2504/2504 [==============================] - 286s 114ms/step - loss: 0.0703 - accuracy: 0.9853 - val_loss: 0.0736 - val_accuracy: 0.9831\n",
      "Epoch 19/40\n",
      "2504/2504 [==============================] - 287s 115ms/step - loss: 0.0572 - accuracy: 0.9888 - val_loss: 0.0635 - val_accuracy: 0.9857\n",
      "Epoch 20/40\n",
      "2504/2504 [==============================] - 291s 116ms/step - loss: 0.0467 - accuracy: 0.9910 - val_loss: 0.0558 - val_accuracy: 0.9873\n",
      "Epoch 21/40\n",
      "2504/2504 [==============================] - 292s 117ms/step - loss: 0.0385 - accuracy: 0.9926 - val_loss: 0.0499 - val_accuracy: 0.9887\n",
      "Epoch 22/40\n",
      "2504/2504 [==============================] - 290s 116ms/step - loss: 0.0320 - accuracy: 0.9939 - val_loss: 0.0455 - val_accuracy: 0.9893\n",
      "Epoch 23/40\n",
      "2504/2504 [==============================] - 291s 116ms/step - loss: 0.0268 - accuracy: 0.9949 - val_loss: 0.0422 - val_accuracy: 0.9899\n",
      "Epoch 24/40\n",
      "2504/2504 [==============================] - 292s 117ms/step - loss: 0.0229 - accuracy: 0.9955 - val_loss: 0.0396 - val_accuracy: 0.9905\n",
      "Epoch 25/40\n",
      "2504/2504 [==============================] - 291s 116ms/step - loss: 0.0198 - accuracy: 0.9961 - val_loss: 0.0372 - val_accuracy: 0.9911\n",
      "Epoch 26/40\n",
      "2504/2504 [==============================] - 288s 115ms/step - loss: 0.0173 - accuracy: 0.9966 - val_loss: 0.0359 - val_accuracy: 0.9912\n",
      "Epoch 27/40\n",
      "2504/2504 [==============================] - 293s 117ms/step - loss: 0.0153 - accuracy: 0.9970 - val_loss: 0.0350 - val_accuracy: 0.9914\n",
      "Epoch 28/40\n",
      "2504/2504 [==============================] - 292s 117ms/step - loss: 0.0136 - accuracy: 0.9973 - val_loss: 0.0341 - val_accuracy: 0.9916\n",
      "Epoch 29/40\n",
      "2504/2504 [==============================] - 292s 117ms/step - loss: 0.0123 - accuracy: 0.9975 - val_loss: 0.0331 - val_accuracy: 0.9918\n",
      "Epoch 30/40\n",
      "2504/2504 [==============================] - 293s 117ms/step - loss: 0.0113 - accuracy: 0.9977 - val_loss: 0.0327 - val_accuracy: 0.9918\n",
      "Epoch 31/40\n",
      "2504/2504 [==============================] - 291s 116ms/step - loss: 0.0102 - accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.9919\n",
      "Epoch 32/40\n",
      "2504/2504 [==============================] - 292s 117ms/step - loss: 0.0094 - accuracy: 0.9981 - val_loss: 0.0323 - val_accuracy: 0.9919\n",
      "Epoch 33/40\n",
      "2504/2504 [==============================] - 290s 116ms/step - loss: 0.0086 - accuracy: 0.9983 - val_loss: 0.0311 - val_accuracy: 0.9921\n",
      "Epoch 34/40\n",
      "2504/2504 [==============================] - 292s 116ms/step - loss: 0.0080 - accuracy: 0.9984 - val_loss: 0.0314 - val_accuracy: 0.9920\n",
      "Epoch 35/40\n",
      "2504/2504 [==============================] - 291s 116ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9921\n",
      "Epoch 36/40\n",
      "2504/2504 [==============================] - 291s 116ms/step - loss: 0.0069 - accuracy: 0.9986 - val_loss: 0.0317 - val_accuracy: 0.9919\n",
      "Epoch 37/40\n",
      "2504/2504 [==============================] - 289s 115ms/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.0316 - val_accuracy: 0.9921\n",
      "Epoch 38/40\n",
      "2504/2504 [==============================] - 289s 115ms/step - loss: 0.0060 - accuracy: 0.9988 - val_loss: 0.0316 - val_accuracy: 0.9920\n",
      "Epoch 39/40\n",
      "2504/2504 [==============================] - 289s 116ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.0318 - val_accuracy: 0.9919\n",
      "Epoch 40/40\n",
      "2504/2504 [==============================] - 291s 116ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.0317 - val_accuracy: 0.9921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1154dc849c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Be2UQ-t_xRlt",
    "outputId": "83facce9-5ef3-435a-d66d-e760c2a5e0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 32s 41ms/step\n",
      "accuracy: 99.09327626228333\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NErB-qCTxfF7",
    "outputId": "9ee9edbf-839d-4336-9d73-b337e3d31edb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n"
     ]
    }
   ],
   "source": [
    "test_samples = [\n",
    "    \"running is very important for me .\".split(),\n",
    "    \"I was running every day for a month .\".split()\n",
    "]\n",
    "print(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_ERjcBOxhcB",
    "outputId": "7c18d30c-8e80-40fa-e1d9-6b45a0d98b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4791 8201 6015 4500   13 3997  781    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]\n",
      " [9799 6752 4791 8972 6644   13 7696  172  781    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "test_samples_X = []\n",
    "for s in test_samples:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    "    test_samples_X.append(s_int)\n",
    " \n",
    "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
    "print(test_samples_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WoS13jrxk6-",
    "outputId": "c445246e-57a0-41f3-c873-189f879eaa1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.1312649e-02 1.3676315e-03 7.5726084e-06 ... 3.2171019e-04\n",
      "   9.4471119e-02 1.5006609e-02]\n",
      "  [3.2329361e-05 4.1634985e-03 3.7154343e-04 ... 9.3281694e-04\n",
      "   5.7492484e-03 2.2881287e-03]\n",
      "  [2.0794707e-06 9.6919978e-01 4.5335910e-04 ... 3.5902788e-04\n",
      "   2.7558289e-04 1.0302132e-03]\n",
      "  ...\n",
      "  [9.9993587e-01 3.5758596e-10 2.5038858e-07 ... 1.6883289e-08\n",
      "   3.1530678e-10 7.7706254e-08]\n",
      "  [9.9989414e-01 5.1052124e-10 4.6374680e-07 ... 3.0207861e-08\n",
      "   4.6220910e-10 1.3385413e-07]\n",
      "  [9.9984145e-01 6.9094658e-10 7.6077265e-07 ... 4.8858876e-08\n",
      "   6.4180855e-10 2.1360246e-07]]\n",
      "\n",
      " [[2.6475548e-04 1.5147317e-04 3.8425822e-04 ... 5.6175991e-06\n",
      "   3.1893787e-06 4.3953173e-03]\n",
      "  [1.4820204e-06 2.6480915e-04 1.0682871e-06 ... 5.3329302e-08\n",
      "   2.8577315e-06 1.7745921e-04]\n",
      "  [5.5112811e-05 4.0267401e-05 5.1188320e-09 ... 5.1641388e-07\n",
      "   8.9390324e-03 3.4340433e-04]\n",
      "  ...\n",
      "  [9.9993587e-01 3.5760711e-10 2.5040745e-07 ... 1.6885609e-08\n",
      "   3.1535369e-10 7.7697067e-08]\n",
      "  [9.9989414e-01 5.1055138e-10 4.6377951e-07 ... 3.0211840e-08\n",
      "   4.6227699e-10 1.3383804e-07]\n",
      "  [9.9984145e-01 6.9098738e-10 7.6082705e-07 ... 4.8865303e-08\n",
      "   6.4190159e-10 2.1357741e-07]]] (2, 271, 47)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "print(predictions, predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZVsZlU6Nxnmk"
   },
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDlzd909xoq-",
    "outputId": "2962b87c-5db6-4683-bc2a-5836b69c3f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['VBG', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "r-XdT-dUxueu"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    " \n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNHBN2PBxve-",
    "outputId": "09ca19eb-1198-4cb8-de93-69077fc89be9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 271, 128)          1290624   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 271, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 271, 47)           24111     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 271, 47)           0         \n",
      "=================================================================\n",
      "Total params: 2,103,215\n",
      "Trainable params: 2,103,215\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    " \n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83cb06OzxyG2",
    "outputId": "db8234a3-45b6-497a-cbec-70956158ae49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amal Zouaq\\Anaconda3\\envs\\keras_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2504 samples, validate on 627 samples\n",
      "Epoch 1/40\n",
      "2504/2504 [==============================] - 268s 107ms/step - loss: 1.1602 - accuracy: 0.9021 - ignore_accuracy: 0.0291 - val_loss: 0.3754 - val_accuracy: 0.9114 - val_ignore_accuracy: 0.1228\n",
      "Epoch 2/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.3380 - accuracy: 0.9084 - ignore_accuracy: 0.0725 - val_loss: 0.3252 - val_accuracy: 0.9072 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "2504/2504 [==============================] - 265s 106ms/step - loss: 0.3154 - accuracy: 0.9097 - ignore_accuracy: 0.0999 - val_loss: 0.3039 - val_accuracy: 0.9191 - val_ignore_accuracy: 0.1367\n",
      "Epoch 4/40\n",
      "2504/2504 [==============================] - 267s 107ms/step - loss: 0.3047 - accuracy: 0.9162 - ignore_accuracy: 0.1326 - val_loss: 0.3083 - val_accuracy: 0.9190 - val_ignore_accuracy: 0.1374\n",
      "Epoch 5/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.2959 - accuracy: 0.9170 - ignore_accuracy: 0.1356 - val_loss: 0.3019 - val_accuracy: 0.9190 - val_ignore_accuracy: 0.1377\n",
      "Epoch 6/40\n",
      "2504/2504 [==============================] - 269s 107ms/step - loss: 0.2919 - accuracy: 0.9168 - ignore_accuracy: 0.1341 - val_loss: 0.3018 - val_accuracy: 0.9191 - val_ignore_accuracy: 0.1387\n",
      "Epoch 7/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.2842 - accuracy: 0.9179 - ignore_accuracy: 0.1419 - val_loss: 0.3001 - val_accuracy: 0.9212 - val_ignore_accuracy: 0.1614\n",
      "Epoch 8/40\n",
      "2504/2504 [==============================] - 271s 108ms/step - loss: 0.2769 - accuracy: 0.9208 - ignore_accuracy: 0.1692 - val_loss: 0.2871 - val_accuracy: 0.9257 - val_ignore_accuracy: 0.2065\n",
      "Epoch 9/40\n",
      "2504/2504 [==============================] - 267s 107ms/step - loss: 0.2706 - accuracy: 0.9239 - ignore_accuracy: 0.2004 - val_loss: 0.2815 - val_accuracy: 0.9273 - val_ignore_accuracy: 0.2246\n",
      "Epoch 10/40\n",
      "2504/2504 [==============================] - 271s 108ms/step - loss: 0.2639 - accuracy: 0.9317 - ignore_accuracy: 0.2835 - val_loss: 0.2754 - val_accuracy: 0.9368 - val_ignore_accuracy: 0.3303\n",
      "Epoch 11/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.2540 - accuracy: 0.9392 - ignore_accuracy: 0.3625 - val_loss: 0.2628 - val_accuracy: 0.9424 - val_ignore_accuracy: 0.3913\n",
      "Epoch 12/40\n",
      "2504/2504 [==============================] - 262s 105ms/step - loss: 0.2388 - accuracy: 0.9456 - ignore_accuracy: 0.4301 - val_loss: 0.2246 - val_accuracy: 0.9484 - val_ignore_accuracy: 0.4488\n",
      "Epoch 13/40\n",
      "2504/2504 [==============================] - 269s 107ms/step - loss: 0.2170 - accuracy: 0.9490 - ignore_accuracy: 0.4644 - val_loss: 0.2014 - val_accuracy: 0.9493 - val_ignore_accuracy: 0.4575\n",
      "Epoch 14/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.1923 - accuracy: 0.9512 - ignore_accuracy: 0.4882 - val_loss: 0.1782 - val_accuracy: 0.9537 - val_ignore_accuracy: 0.5047\n",
      "Epoch 15/40\n",
      "2504/2504 [==============================] - 267s 106ms/step - loss: 0.1687 - accuracy: 0.9563 - ignore_accuracy: 0.5418 - val_loss: 0.1578 - val_accuracy: 0.9590 - val_ignore_accuracy: 0.5623\n",
      "Epoch 16/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.1478 - accuracy: 0.9618 - ignore_accuracy: 0.6008 - val_loss: 0.1393 - val_accuracy: 0.9633 - val_ignore_accuracy: 0.6071\n",
      "Epoch 17/40\n",
      "2504/2504 [==============================] - 267s 106ms/step - loss: 0.1304 - accuracy: 0.9666 - ignore_accuracy: 0.6533 - val_loss: 0.1239 - val_accuracy: 0.9689 - val_ignore_accuracy: 0.6674\n",
      "Epoch 18/40\n",
      "2504/2504 [==============================] - 267s 107ms/step - loss: 0.1117 - accuracy: 0.9727 - ignore_accuracy: 0.7143 - val_loss: 0.1081 - val_accuracy: 0.9729 - val_ignore_accuracy: 0.7110\n",
      "Epoch 19/40\n",
      "2504/2504 [==============================] - 272s 109ms/step - loss: 0.0942 - accuracy: 0.9778 - ignore_accuracy: 0.7674 - val_loss: 0.0928 - val_accuracy: 0.9771 - val_ignore_accuracy: 0.7546\n",
      "Epoch 20/40\n",
      "2504/2504 [==============================] - 272s 109ms/step - loss: 0.0775 - accuracy: 0.9828 - ignore_accuracy: 0.8197 - val_loss: 0.0793 - val_accuracy: 0.9815 - val_ignore_accuracy: 0.8030\n",
      "Epoch 21/40\n",
      "2504/2504 [==============================] - 273s 109ms/step - loss: 0.0631 - accuracy: 0.9871 - ignore_accuracy: 0.8656 - val_loss: 0.0679 - val_accuracy: 0.9850 - val_ignore_accuracy: 0.8404\n",
      "Epoch 22/40\n",
      "2504/2504 [==============================] - 268s 107ms/step - loss: 0.0512 - accuracy: 0.9903 - ignore_accuracy: 0.8985 - val_loss: 0.0590 - val_accuracy: 0.9870 - val_ignore_accuracy: 0.8616\n",
      "Epoch 23/40\n",
      "2504/2504 [==============================] - 267s 107ms/step - loss: 0.0417 - accuracy: 0.9921 - ignore_accuracy: 0.9172 - val_loss: 0.0523 - val_accuracy: 0.9882 - val_ignore_accuracy: 0.8746\n",
      "Epoch 24/40\n",
      "2504/2504 [==============================] - 266s 106ms/step - loss: 0.0345 - accuracy: 0.9934 - ignore_accuracy: 0.9312 - val_loss: 0.0473 - val_accuracy: 0.9890 - val_ignore_accuracy: 0.8838\n",
      "Epoch 25/40\n",
      "2504/2504 [==============================] - 269s 107ms/step - loss: 0.0289 - accuracy: 0.9944 - ignore_accuracy: 0.9411 - val_loss: 0.0436 - val_accuracy: 0.9896 - val_ignore_accuracy: 0.8898\n",
      "Epoch 26/40\n",
      "2504/2504 [==============================] - 268s 107ms/step - loss: 0.0247 - accuracy: 0.9952 - ignore_accuracy: 0.9494 - val_loss: 0.0409 - val_accuracy: 0.9900 - val_ignore_accuracy: 0.8939\n",
      "Epoch 27/40\n",
      "2504/2504 [==============================] - 270s 108ms/step - loss: 0.0214 - accuracy: 0.9957 - ignore_accuracy: 0.9553 - val_loss: 0.0387 - val_accuracy: 0.9904 - val_ignore_accuracy: 0.8982\n",
      "Epoch 28/40\n",
      "2504/2504 [==============================] - 268s 107ms/step - loss: 0.0187 - accuracy: 0.9962 - ignore_accuracy: 0.9600 - val_loss: 0.0378 - val_accuracy: 0.9905 - val_ignore_accuracy: 0.9000\n",
      "Epoch 29/40\n",
      "2504/2504 [==============================] - 268s 107ms/step - loss: 0.0167 - accuracy: 0.9965 - ignore_accuracy: 0.9629 - val_loss: 0.0363 - val_accuracy: 0.9908 - val_ignore_accuracy: 0.9024\n",
      "Epoch 30/40\n",
      "2504/2504 [==============================] - 271s 108ms/step - loss: 0.0150 - accuracy: 0.9969 - ignore_accuracy: 0.9674 - val_loss: 0.0349 - val_accuracy: 0.9911 - val_ignore_accuracy: 0.9056\n",
      "Epoch 31/40\n",
      "2504/2504 [==============================] - 271s 108ms/step - loss: 0.0135 - accuracy: 0.9971 - ignore_accuracy: 0.9697 - val_loss: 0.0348 - val_accuracy: 0.9911 - val_ignore_accuracy: 0.9069\n",
      "Epoch 32/40\n",
      "2504/2504 [==============================] - 270s 108ms/step - loss: 0.0124 - accuracy: 0.9974 - ignore_accuracy: 0.9726 - val_loss: 0.0336 - val_accuracy: 0.9914 - val_ignore_accuracy: 0.9093\n",
      "Epoch 33/40\n",
      "2504/2504 [==============================] - 270s 108ms/step - loss: 0.0113 - accuracy: 0.9977 - ignore_accuracy: 0.9754 - val_loss: 0.0329 - val_accuracy: 0.9916 - val_ignore_accuracy: 0.9105\n",
      "Epoch 34/40\n",
      "2504/2504 [==============================] - 292s 117ms/step - loss: 0.0104 - accuracy: 0.9978 - ignore_accuracy: 0.9776 - val_loss: 0.0331 - val_accuracy: 0.9915 - val_ignore_accuracy: 0.9091\n",
      "Epoch 35/40\n",
      "2504/2504 [==============================] - 300s 120ms/step - loss: 0.0098 - accuracy: 0.9980 - ignore_accuracy: 0.9789 - val_loss: 0.0325 - val_accuracy: 0.9917 - val_ignore_accuracy: 0.9123\n",
      "Epoch 36/40\n",
      "2504/2504 [==============================] - 307s 123ms/step - loss: 0.0090 - accuracy: 0.9982 - ignore_accuracy: 0.9808 - val_loss: 0.0323 - val_accuracy: 0.9919 - val_ignore_accuracy: 0.9140\n",
      "Epoch 37/40\n",
      "2504/2504 [==============================] - 286s 114ms/step - loss: 0.0083 - accuracy: 0.9983 - ignore_accuracy: 0.9821 - val_loss: 0.0318 - val_accuracy: 0.9919 - val_ignore_accuracy: 0.9140\n",
      "Epoch 38/40\n",
      "2504/2504 [==============================] - 285s 114ms/step - loss: 0.0077 - accuracy: 0.9985 - ignore_accuracy: 0.9839 - val_loss: 0.0319 - val_accuracy: 0.9920 - val_ignore_accuracy: 0.9144\n",
      "Epoch 39/40\n",
      "2504/2504 [==============================] - 272s 109ms/step - loss: 0.0072 - accuracy: 0.9985 - ignore_accuracy: 0.9847 - val_loss: 0.0323 - val_accuracy: 0.9919 - val_ignore_accuracy: 0.9150\n",
      "Epoch 40/40\n",
      "2504/2504 [==============================] - 270s 108ms/step - loss: 0.0068 - accuracy: 0.9986 - ignore_accuracy: 0.9858 - val_loss: 0.0322 - val_accuracy: 0.9920 - val_ignore_accuracy: 0.9146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x115348f1f48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tv3fLOy9x0Lj",
    "outputId": "db220ae7-fcbc-4bc9-8502-0016d5456111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NNS', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_samples_X)\n",
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 52s 66ms/step\n",
      "ignore_accuracy: 90.68601727485657\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore_accuracy: 90.68601727485657\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model.metrics_names[2]}: {scores[2] * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
